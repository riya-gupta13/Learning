
Consistency Lvels:
two apps in diff regions=partition keys then leader and then followers diff in both
one app changes sme data this has to be replicated on othr region.
thre mybe one option that u may miss sme data to replicate bcoz of concurrent users
so, one way is you wait tht whn all is done thn u replicate
this is Consistency:
strong= it will replicate only whn change is recent
Eventual: you miss latency in string but here u do not, you loose performace but win on consistency
if app read then it may read older or new so u may win on performace but loose on consistency
Boundless Staleness:here reads can lag behind the writes b
session: here reads can be guranteed for same session.
Consistent Prefix:here thre is a delay in raeds of most recent data but you will nvr see out of order writes
means it always see it inorder

In azure:default consistency=thre is limitation on ths based on locations ur app is

Azure Fxn-Always on Setting:
when new or updated blob is detected(Blob storage trigger)
azure Fxns=(based on comsuption plan, app service plan, basic/standartd)(Always on settings)
in azure=new rsrc=azure fxn=plan type(app service plan0=create
in fxns app=config=general settings=Always on(on)(based on blob trigger)
thre will be small dlay in doing tht thn to avoid tht use this always on setting inj appsrvice plan

MANAGED ENTITIES:
THIS HLPS  to auithncate to services tht support azure AD authntication
u can do basede on access policy or key vault=u hva to add some secrets md pswrd thn only u can do it
thn we use mngd entity:azure ad=rolebased control=azure storge acc
hre u dnt hve to add any secrets fr tht u can directly add this

Authentication and Authorization:
Authntication: the process of proving that you r who u say u r
Autho=granting access to authntictd user to access resrc
azureAd=IdentityProvider
You have to modify the manifest file to get the group claims
If you are going to generate a new application in Azure AD, you also have to ensure to change the application manifest file to ensure the group membership claims are returned to the web application.

You have to develop a web application that will run on the Azure Web App service. Here users of the web application will authenticate by using their Azure AD credentials.
The user’s permission level would be determined by the Azure AD group membership. You have to configure the authorization for the web application.
You decide to implement the following steps
1) Create a new Azure AD application manifest.
2) You set the value of groupMembershipClaims option to All
3) In the website, you use the value of the groups claim from the JWT for the user to determine the permissions

Resource Owner=issues Auth code and Authen
the user who wld be able to access the protctd resrc
Res Server: this is server that is hosting the protected resrcs. Access is granted to the resrc with use of access token
Auth Server:The server that will issue the access tokens

For an application defined in Azure AD to 
access a storage account, you have to implement the permission of user_impersonation. 
Here the permission of the logged-on user would be used to access the storage account.
Yes, this can also be achieved via role claims. The required app role can be created at the application level and then assigned to users

Azure Storage API: delegated type permission
GraphAPI: User.Read
Microsoft Graph API: delegated

APPLICATION INSIGHTS:
performance mngmt service:
monitor
detect anomalies
diagonose isssues
how users use appln
improve perf nd usabilty of app
to mke it wrk:install small pckg

Req rates,response times and failure rates
exception page views nd load performance 
user and session counts
perf counters
diagnose tace logs
any custom evnts or metrics

Appl Insights
You can set a daily cap to ensure that the cost does not exceed a budget. You can go to “Usage and estimated costs” for your Application Insights resource and then hit on Daily cap.
funnel: is the progreesion of the series of steps in an app
azure=funnels=edit=selct the pages u wnt=view tab

User Flows:wht hppns bfore and after=u can go to user flow=it will mke flowchart

Retention:how mnyusers return back to ur app

Impact=slct event=edit page=u can see impact of one page on anthr

Appln Map=it shos diff components in ur app

Availabilty=u can add test=create test=URL PING=copy url of app nd pste=test frq
Availabilty=respn time=from diff loc

CDN Cache:
Bypass Cache; do not cache, even cache control exixts
Override:set the cache time evn if server sets the cache control setting
Here since videos of varying quality must be cache, that means every unique URL must be cached.
Setif missing: if cache control setting is present, respect the setting. if stng is not present then set cache time

Azure FrontDoor Service:
hpls acclerate ur app perfrmnce by routing traffic on perf of ur req
service wrks at lyr 7 or HTTP/HTTPS
will route clinet your client req to fstest nd most avalble app backnd
an app bckned ia ny internt facng service that could be hosted inside or outside azure.

You don’t need to purge the cache assets. Here the issue is that the file size needs to be less than 8MB
Yes, the compression type is supported in Azure front door service, the issue is that the file size needs to be less than 8MB

URL BASES: YOU CAN ROUTR Ttraffic based on server based on urls
Mutiple site:u cvan configure more thn one website on sme front door
Session affinity: u can keep user session attchd to sme bcknd
SSl termination
web app firewall

Trnsient Faults:
we hve to mke our services transient faults
Having shared env=azure cloud env shared between multiple customers.it can load too much, so it should hndle all ths your app must hndle these
the app should retry or hve appropriate startegu fr retry .NEt sdk services already has facilities of hndlinf. eg.CosmosDB
The DocumentClient class automatically retry failed attmpts, u can set max wait time for each retry nd no. of retries.
you can use exponential backoff.

Messsage Lock Duration
Peek and lock is one mthd=u get msg, proceess it and thn del it explicitly
whn one app picks one msg thn its in lock state so tht no app picks it and thn its del thn othr app can pick anothr
if the msg is not prccesed, thn it again goes back to queue and the app can again pick and process it
Receive and del is anothr mthod where frst its del thn received

Time to Live Feature:
in overview in queues, by default its 14 days so tht consumer can consume it we can chnge tht or we can set this fr each msg also frm prgrm.

Dead LtterQueue:
app sending msgs to queue=if any issues in processing th n thy r in dead lttr queue(tht were nt processed ot time to live)
we can enable it in overview in Queues=DeadLettering

Duplicate Message Detection:
create a new queue= enable duplicate detection
sender send msg to queue, sender crashes, it mght send msg again
so that duplicate msgs are nt present, we can enable, queue will identify tht thsi msg came bfore or not but frf this we hve to provide msg a msg id, write code r tht in vs.
u have tp specify messageId in code.

Service Bus Message Prop:
Id=ReplyTo=is used if u want to receive the response from the queue thn u can ujse ReplyTo in which we can store these messages in anothr queue(responsequeue)

CorrelationID= it correlates wht has been sent and received

AzureServuce Bus topics:
app= sending msgs to topic = in topic we can subscribe=each app needs to subscribe, if they subscribe, thy will receive msgs fr tht, thy can use filters to decide which msgs to receive

Azure Service Bus Topics-Filters:
SQL Filters=SQl like condtions
system-defined prop=must prefixed with sys
Boolean Filters: this is a TrueFiler, FalseFilter
Correlation Filters: conditiond can be used to match against user's or sys prop
Crr filters faster thn SQL filters.

Azure Service Bus Queue and Azure Storage queue
More thn 80 Gb of msgs thn use azure storage othrwise ui can use service bsu
in this service bus: you need guranteed frstin frstout delivery of msgs
for this u can enable duplicate detection
u need to enable sessions fr queue
Azure Storage Queues don’t have the facility to support first-in-first-out ordering of messages

Azure event gid has support sonly fro Blob storage acc
if u have high sacle, thn use evnt grid, 10000(blobs)

Event Hubs: big data streaming platfrm
produces,client=in multiple prtition(db)=consumer grps(tht read data)

------------------------------------------------------------------------------------------------------------------------------------------------------------

All backups are stored in the recovery services vault.

For an application defined in Azure AD to access a storage account, you have to implement the permission of user_impersonation. Here the permission of the logged-on user would be used to access the storage account.

Here since we want to minimize changes to Azure AD ,we can create one user-assigned managed identity and assign it to all of the Azure Web Apps and functions. We can then gives the required access to the managed identity to the resources in the key vault

You can set a daily cap to ensure that the cost does not exceed a budget. You can go to “Usage and estimated costs” for your Application Insights resource and then hit on Daily cap.

Yes, the MIME type is supported, the issue is that the file size needs to be less than 8MB

Change feed doesnt captures deletes and is enabled by default in Cosmos DB and doesnt gurantees order

You can use the validate-jwt policy to validate the JSON Web tokens for Open ID Connect authentication.

Make use of your Integration Accounts and the Enterprise Integration Pack to integrate your B2B apps with Azure Logic Apps.

To use an existing API (which uses an Open API specification) behind the Azure API Management service , you can use the Import-AzApiManagementApi command.

If you are going to generate a new application in Azure AD, you also have to ensure to change the application manifest file to ensure the group membership claims are returned to the web application.

Since this is a React application which is based as a single page application, we have to ensure that the setting for oauth2AllowIdTokenImplicitFlow is set to true. This would allow tokens to be passed when it comes to using the Implicit Flow workflow.

Since all of the services must be available from a virtual network ,we have to mention the osType as Linux. Currently this is the only OS that supports container instances to be available from a virtual network.

When it comes to monitoring Azure container instances for CPU percentage, the metric that needs to be used is CPU Usage. And in the YAML deployment, 1 CPU is being used for the underlying container instance. Also, when it comes to CPU utilization for Azure container instances, they are measured in millicores. We have to create an alert when the CPU utilization reaches 80%, that means 0.8 CPU, which means 800 millicores. 

If you are looking at millions of agreements per hour, you need to use a data ingestion service like the Azure Event Hub. You can use Event Capture to store the agreements into Azure blob storage for long term storage.

The sequence of steps are to create a new integration account in Azure, Link the Logic App to the integration account, Add partners, schemas, certificates, maps and agreements and Create a custom connector for the Logic App.

If you want to see errors that can occur in Azure Functions locally, you can associate the Azure Functions code in Visual Studio with an Application Insights resources in Azure. Here you can use Live metrics stream to see the requests and any exceptions which occur.

The integration is available with the Premium tier of Azure service bus

The first statement in the Dockefile must be the FROM statement to specify the image to use as the base image.
Then specify the Image working directory
Then copy all of the application contents using the COPY command
And then use the CMD command to run the PowerShell command and the ENTRYPOINT statement to run the dotnet application.The sequence of steps would be follows
FROM mcr.microsoft.com/dotnet/core/sdk:3.1
WORKDIR /apps/demoapp
COPY ./.
CMD powershell ./setup.ps1
ENTRYPOINT [“dotnet”,” demoapp.dll”]

Here you would only create one Azure Service Bus namespace
This is how the workflow would look like
Remember that an Order is pertinent to a particular restaurant ONLY.
So, an application can send an Order to a particular topic which represents a topic.
The driver can subscribe to the restaurant or topic here and receive only orders from that topic.

You can use Service Bus topics and Event Grid for implementing a publish-subscribe model

On the virtual machine itself , you can use PowerShell to make a request to the local managed identity to get the access token. You can then use the access token to call Azure Resource Manager.

When you look at the pricing plans for the Azure API management, when it comes to the Consumption plan, it only supports external cache.

You can use Helm for the deployment of services on Kubernetes.

You can use the kubectl tool to view the public IP address that is assigned to the service

For MongoDB, if you need a migration with minimum downtime, you need to implement online migration using the Azure Database Migration Service

An Integration Service Environment is a fully isolated and dedicated environment for all enterprise-scale integration needs. 
When you create a new Integration Service Environment, it is injected into your Azure virtual network, 
which allows you to deploy Logic Apps as a service on your VNET.

// Stop the Virtual Machine
Stop-AzVM -ResourceGroupName $rgName -Name $vmName -Force
// Get a handle to the virtual machine
$vm = Get-AzVM -Name $vmName -ResourceGroupName $rgName
// Create an image configuration
$image = New-AzImageConfig -Location $location -SourceVirtualMachineId $vm.Id
// Create the custom image
New-AzImage -Image $image -ImageName $imageName -ResourceGroupName $rgName
Steps are:
stop rsrcgrp
set image
get
define image
newimage



First set all of the variables
Then create the Azure Web App
Then set the docker image which will be deployed as a container
Then set the DNS name

The Microsoft documentation gives two ways in which you can add a warm-up behavior for your wen applications. 
One is to add the applicationInitialization configuration element to the web.config file. 
And the other is to add app settings for your web application

On the virtual machine itself , you can use PowerShell to make a request to the local managed identity to get the access token. 
You can then use the access token to call Azure Resource Manager.

For using feature flags, you need to make use of the Azure App Configuration service. 
To ensure the configuration settings are refreshed without the need to restart the web app, you can use the middleware of UseAzureAppConfiguration();

The types of availability tests available are URL ping test and Multi-step web test.

When a blob is moved to the archive access tier, it needs to be rehydrated before it can be accessed. 
For this you have two levels of priority when it comes to rehydrating the blob. 
With the standard priority, the rehydration process can take up to 15 hours. 
For the high priority rehydration process, the job can take an hour for objects under 10 GB in size.

When it comes to an HTTP trigger, the maximum function timeout is 230 seconds. For long-running functions with HTTP triggers,you should use Durable Fxns.

Role-based access control is used for authorization and not authentication.

Here since we need all calls to be securely access via the use of Azure AD, we can make use of managed identities in Azure AD.

Here since we need Azure AD resources to remain even when the Logic App gets deleted, we need to make use of a user-assigned identity rather than the system-assigned identity.

To securely access your backend resources via the API gateway instance, you can make use of client certificates that can be stored in Azure key vault.

When you look at the pricing plans for the Azure API management, when it comes to the Consumption plan, it only supports external cache.

Cosmos DB Operator - Lets you manage Azure Cosmos DB accounts, but not access data in them. Prevents access to account keys and connection strings

1. AcquireLeaseAsync does not specify leaseTime.
leaseTime is a TimeSpan representing the span of time for which to acquire the lease, which will be rounded down to seconds. If null, an infinite lease will be acquired.
2.The GetBlockBlobReference method just gets a reference to a block blob in this container
3. BreakLeaseAsync - breakPeriod parameter - A TimeSpan representing the amount of time to allow the lease to remain, 
which will be rounded down to seconds. In this case, it is zero. so it releases the lease.

The correct order is
1.Prepare data or declare variables.
2. Create the web app
3. Set the container
4.Configure custom domain.

The correct sequence of activities are
# Create a resource group
#Create an App Service plan
#Create a web app
#Create a deployment slot with the name "staging"
# Deploy sample code to "staging" slot from GitHub

Front Door caches assets until the asset's time-to-live (TTL) expires. Whenever a client requests an asset with expired TTL, 
the Front Door environment retrieves a new updated copy of the asset to serve the request and then stores the refreshed cache. 
Sometimes you may wish to purge cached content from all edge nodes and force them all to retrieve new updated assets. 
Single path purge: Purge individual assets by specifying the full path of the asset (without the protocol and domain), 
with the file extension, for example, /pictures/strasbourg.png;
The scenario in the question asks to purge individual and specific files. So, single path purge is the right choice.

The managed identity policy essentially uses the managed identity to obtain an access token from Azure Active Directory for accessing the specified resource. After successfully obtaining the token, the policy will set the value of the token in the Authorization header using the Bearer scheme.
For managed identity, we no need to send any credentials.

The Azure Functions Premium plan (sometimes referred to as Elastic Premium plan) is a hosting option for function apps. 
The Premium plan provides features like VNet connectivity, no cold start, and premium hardware.

A managed identity from Azure Active Directory (Azure AD) allows your app to easily access other Azure AD-protected resources such as Azure Key Vault. 
The identity is managed by the Azure platform and does not require you to provision or rotate any secrets.
Your application can be granted two types of identities:
A system-assigned identity is tied to your application and is deleted if your app is deleted. An app can only have one system-assigned identity.
A user-assigned identity is a standalone Azure resource that can be assigned to your app. An app can have multiple user-assigned identities.

Subscribers can define which messages they want to receive from a topic. These messages are specified in the form of one or more named subscription rules. 
Each rule consists of a condition that selects particular messages and an action that annotates the selected message. 
For each matching rule condition, the subscription produces a copy of the message, which may be differently annotated for each matching rule.
Each newly created topic subscription has an initial default subscription rule. 
If you don't explicitly specify a filter condition for the rule, the applied filter is the true filter that enables all messages to be selected into the subscription.

groupMembershipClaims will get the all of the security groups, distribution groups, and Azure AD directory roles that the signed-in user is a member of. As website personalization is based on user group memberships, this must be configured.

oauth2AllowImplicitFlow flag is used for browser-based apps, like JavaScript single-page apps. 
Specifies whether this web app can request OAuth2.0 implicit flow access tokens

You can route events in Event Grid directly to Service Bus queues for use in buffering or command & control scenarios in enterprise applications.

While developing an application, you often want to see what's being written to the logs in near real time when running in Azure.
There are two ways to view a stream of the log data being generated by your function executions.
Built-in log streaming: the App Service platform lets you view a stream of your application log files. 
This stream is equivalent to the output seen when you debug your functions during local development and 
when you use the Test tab in the portal. All log-based information is displayed.
Live Metrics Stream: when your function app is connected to Application Insights, 
you can view log data and other metrics in near real time in the Azure portal using Live Metrics Stream.

User authentication flow-
The following steps detail the user authentication process:
1. The user selects Sign in in the website.
2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.
3. The user signs in.
4. Azure AD redirects the user's session back to the web application. The URL includes an access token.
To redirect user for authentication when user clicks on Sign in, we need Azure AD endpoint URI and Azure AD tenant ID.

Scenario:  Azure Event Grid must use Azure Service Bus for queue-based load leveling.
Events in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.

Service Bus can now emit events to Event Grid when there are messages in a queue or a subscription when no receivers are present. 
You can create Event Grid subscriptions to your Service Bus namespaces, listen to these events, and then react to the events by starting a receiver.
To enable the feature, you need the following items:
· A Service Bus Premium namespace with at least one Service Bus queue or a Service Bus topic with at least one subscription.
· Contributor access to the Service Bus namespace.
· Additionally, you need an Event Grid subscription for the Service Bus namespace.

Event Grid - Event Grid is an eventing backplane that enables event-driven, reactive programming. 
It uses a publish-subscribe model. Publishers emit events, but have no expectation about which events are handled. 
Subscribers decide which events they want to handle.
Service Bus Topics and subscriptions enable relationships between publishers and subscribers.

Scenario: File access must restrict access by IP, protocol, and Azure AD rights.
Auditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, 
stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.
A SAS token for access to a container, directory, or blob may be secured by using either Azure AD credentials or an account key. 
A SAS secured with Azure AD credentials is called a user delegation SAS. 
Microsoft recommends that you use Azure AD credentials when possible as a security best practice, rather than using the account key, 
which can be more easily compromised.

The purpose of the change feed is to provide transaction logs of all the changes that occur to the blobs and the blob metadata in your storage account. 
The change feed provides ordered, guaranteed, durable, immutable, read-only log of these changes. 
Client applications can read these logs at any time, either in streaming or in batch mode. 
The change feed enables you to build efficient and scalable solutions that process change events that occur in your Blob Storage account at a low cost.

You can access the console logs generated from inside the container.
First, turn on container logging by running the following command:
az webapp log config --name <app-name> --resource-group <resource-group-name> --docker-container-logging filesystem
Replace <app-name> and <resource-group-name> with the names appropriate for your web app.
Once container logging is turned on, run the following command to see the log stream:
az webapp log tail --name <app-name> --resource-group <resource-group-name>
If you don't see console logs immediately, check again in 30 seconds
To stop log streaming at any time, type Ctrl+C.

The requirement is to minimize the latency. If your function app is on the Consumption plan, 
there can be up to a 10-minute delay in processing new blobs if a function app has gone idle. 
To avoid this latency, you should use an App Service plan with Always On enabled.

Azure event hub does not support duplicate detection and Azure storage queue does not provide transactional support.

IdentityType - The type of identity used for the virtual machine. Valid values are SystemAssigned, UserAssigned, SystemAssignedUserAssigned, and None.
SystemAssigned - This parameter enables the VM with the system-assigned identity

GetPropertiesAsync - Get the blob's properties and metadata
Metadata.Add - Add metadata to the dictionary by calling the Add method
SetMetadataAsync - Set the blob's metadata.

AzCopy is a command-line utility that you can use to copy files to or from a storage account.

If your application involves multiple stages, you need to know if most customers are progressing through the entire process, 
or if they are ending the process at some point. The progression through a series of steps in a web application is known as a funnel. 
You can use Azure Application Insights Funnels to gain insights into your users, and monitor step-by-step conversion rates.

Impact analyzes how load times and other properties influence conversion rates for various parts of your app.

The retention feature in Azure Application Insights helps you analyze how many users return to your app, 
and how often they perform particular tasks or achieve goals. 
For example, if you run a game site, you could compare the numbers of users who return to the site 
after losing a game with the number who return after winning.

The User Flows tool visualizes how users navigate between the pages and features of your site. It's great for answering questions like:
· How do users navigate away from a page on your site?
· What do users click on a page on your site?
· Where are the places that users churn most from your site?
· Are there places where users repeat the same action over and over?

The Isolated service plan is designed to run mission-critical workloads that are required to run in a virtual network. 
The Isolated plan allows customers to run their apps in a private,dedicated environment in an Azure data Centre using Dv2-series VMs with faster processors,
SSD storage and double the memory-to-core ratio compared to Standard.

There are varying password length requirements, depending on the tool you are using:
• Portal - between 12 - 72 characters
• PowerShell - between 8 - 123 characters
• CLI - between 12 - 123
• Have lower characters
• Have upper characters
• Have a digit
• Have a special character (Regex match [\W_])
The following passwords are not allowed:
abc@123 iloveyou! P@$$w0rd P@ssw0rd P@ssword123
Pa$$word pass@word1 Password! Password1 Password22

If WEBSITES_ENABLE_APP_SERVICE_STORAGE setting is unspecified or set to true, the /home/ directory will be shared across scale instances, 
and files written will persist across restarts. Explicitly setting WEBSITES_ENABLE_APP_SERVICE_STORAGE to false will disable the mount.

Lock down inbound traffic to your Azure Virtual Machines with Azure Security Center's just-in-time (JIT) virtual machine (VM) access feature. 
This reduces exposure to attacks while providing easy access when you need to connect to a VM.

The Dockerfile is a text file that contains the instructions needed to create a new container image. 
These instructions include identification of an existing image to be used as a base, 
commands to be run during the image creation process, and a command that will run when new instances of the container image are deployed.

Platform logs and metrics can be sent to the destinations
Log Analytics
Storage Account
Event Hub

if you want to customize the deployment process, for example you want to run your tests before deploying (or after) and cancel the deployment if they fail?
That's what the custom deployment feature is about, you just need to add a file to the root of your repository with the name .deployment and the content:
[config]
command = YOUR COMMAND TO RUN FOR DEPLOYMENT
this command can be just running a script (batch file) that has all that is required for your deployment, 
like copying files from the repository to the web root directory for example.

API Management allows you to secure access to the back-end service of an API using client certificates. 
The below link shows how to manage certificates in the Azure API Management service instance in the Azure portal. 
It also explains how to configure an API to use a certificate to access a back-end service.

Time-based retention policy support allows Users can set policies to store data for a specified interval. 
When a time-based retention policy is set, blobs can be created and read, but not modified or deleted. 
After the retention period has expired, blobs can be deleted but not overwritten

For business-to-business (B2B) solutions and seamless communication between organizations, 
you can build automated scalable enterprise integration workflows by using the Enterprise Integration Pack (EIP) with Azure Logic Apps. 
Although organizations use different protocols and formats, they can exchange messages electronically. 
The EIP transforms different formats into a format that your organizations' systems can process and supports industry-standard protocols, 
including AS2, X12, and EDIFACT.

Yes, Azure Cache for Redis can be used to store Session data.
Azure Cache for Redis provides an in-memory data store based on the open-source software Redis. 
Redis improves the performance and scalability of an application that uses on backend data stores heavily.
Azure Cache for Redis can be used as a distributed data or content cache, a session store, a message broker, and more.

The correct script is 

1.Get-AzSubscription
2.Set-AzContext -SubscriptionId $subscriptionID
3.Get-AzStorageAccountKey -ResourceGroupName $resGroup -Name $storAcct
4.$secretvalue = ConvertTo-SecureString $storAcctKey -AsPlainText -Force
Set-AzKeyVaultSecret -VaultName $vaultName -Name $secretName -SecretValue $secretvalue
5.Get-AzKeyVaultSecret -VaultName $vaultName

Contributor - Can create and manage all types of Azure resources but can't grant access to others

Session is the best consistency setting for user data that contains shopping basket information. 
Session consistency will ensure that every item the user put in their basket is displayed when they review their basket

To connect on-premises data sources from Azure Logic Apps, download and install the on-premises data gateway on a local computer. 
The gateway works as a bridge that provides quick data transfer and encryption between data sources on premises and your logic apps.

Scenario: The Shipping Logic App requires secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.
If your logic apps need access to secured resources, such as virtual machines (VMs) and other systems or services, 
that are inside or connected to an Azure virtual network you can create an integration service environment (ISE).
An ISE is an instance of the Logic Apps service that uses dedicated resources and runs separately from the "global" multi-tenant Logic Apps service.

Deployments view provides history of deployments happened in a resource group along with last modified date.

The ARM template has the following sections:
Parameters - Provide values during deployment that allow the same template to be used with different environments.
Variables - Define values that are reused in your templates. They can be constructed from parameter values.
User-defined functions - Create customized functions that simplify your template.
Resources - Specify the resources to deploy.
Outputs - Return values from the deployed resources.

docker push is the command to push an image to Azure Container registry. 
The correct format is docker push <registryname>.azureacr.io/<namespace>/<imagename>

To access on-premise service from Azure app service, you need to configure VNet Integration. 
Your apps need to be in a Standard, Premium, or PremiumV2 App Service plan for VNet configurations.

You need to set WEBSITE_RUN_FROM_PACKAGE=1
There are several benefits to running directly from a package:
Eliminates file lock conflicts between deployment and runtime.
Ensures only full-deployed apps are running at any time.
Can be deployed to a production app (with restart).
Improves the performance of Azure Resource Manager deployments.
May reduce cold-start times, particularly for JavaScript functions with large npm package trees.

ARR affinity: In a multi-instance deployment, ensure that the client is routed to the same instance for the life of the session. 
You can set this option to Off for stateless applications. If client is routed to different instances, then session data will be lost.

Soft delete protects blob data from being accidentally or erroneously modified or deleted. 
When soft delete is enabled for a storage account, blobs, blob versions (preview), 
and snapshots in that storage account may be recovered after they are deleted, within a retention period that you specify.

The Start-AzureStorageBlobCopy cmdlet starts to copy a blob

Register your application with your Azure Active Directory (Azure AD) tenant. This will give you an Application ID for your application, as well as enable it to receive tokens.
At the time of registration, Provide the Redirect URI. For web applications, this is the base URL of your app where users can sign in. 
For example, http://localhost:12345. For public client (mobile & desktop), Azure AD uses it to return token responses. 
Enter a value specific to your application. For example, http://MyFirstAADApp

For global and custom caching rules, you can specify the following Caching behavior settings:
Bypass cache: Do not cache and ignore origin-provided cache-directive headers.
Override: Ignore origin-provided cache duration; use the provided cache duration instead. This will not override cache-control: no-cache.
Set if missing: Honor origin-provided cache-directive headers, if they exist; otherwise, use the provided cache duration.

To monitor and respond to specific events that happen in Azure resources or third-party resources, 
you can automate and run tasks as a workflow by creating a logic app that uses minimal code. 
These resources can publish events to an Azure event grid. In turn, the event grid pushes those events to subscribers that have queues, webhooks, 
or event hubs as endpoints. As a subscriber, your logic app can wait for those events from the event grid before running automated workflows to perform tasks.

Anonymous allows public read access.

Cache-store-value is part of both Inbound and Outbound section. Since, we need to cache the response for the user ID, it should be on Outbound.

Azure Notification Hub is used to Send push notifications from any backend to any mobile device.
Azure Notification Hubs need to be used for sending notifications across to devices.

The Service Bus is for high-value enterprise messaging, and is used for order processing and financial transactions.

Azure Event Grid is for managing routing of all events from any source to any destination and event size limit is 1MB

The az container attach command provides diagnostic information during container startup.
Once the container has started, it also writes standard output and standard error streams to your local terminal.
The az container attach command shows container events and logs. By contrast, the az container logs only shows the logs and not the startup events.

There are cases where you may have to set multiple rules in a profile. 
The following autoscale rules are used by the autoscale engine when multiple rules are set.
On scale-out, autoscale runs if any rule is met. On scale-in, autoscale require all rules to be met.

The following triggers have built-in retry support:
Azure Blob storage
Azure Queue storage
Azure Service Bus (queue/topic)
By default, these triggers retry requests up to five times. After the fifth retry, 
both the Azure Queue storage and Azure Service Bus triggers write a message to a poison queue.

Durable Functions is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. 
The primary use case for Durable Functions is simplifying complex, stateful coordination requirements in serverless applications.  
In the function chaining pattern, a sequence of functions executes in a specific order. 
In this pattern, the output of one function is applied to the input of another function. 
You can use Durable Functions to implement the function chaining pattern.

The need to reuse existing CQL queries means that Cassandra is the best choice for in this scenario.

The cost of a read operation (in terms of RUs consumed) with the eventual consistency level is the lowest of all the Azure Cosmos DB consistency levels.
Eventual: There's no ordering guarantee for reads. In the absence of any further writes, the replicas eventually converge.
This provides low latency and high availability reads but doese nt guarantee the order and the new writes.

For Archive Access:
Standard priority: The rehydration request will be processed in the order it was received and may take up to 15 hours.
High priority: The rehydration request will be prioritized over Standard requests and may finish in under 1 hour for objects under ten GB in size.

Ephemeral OS disks are created on the local virtual machine (VM) storage and not saved to the remote Azure Storage. 
Ephemeral OS disks work well for stateless workloads, where applications are tolerant of individual VM failures, 
but are more affected by VM deployment time or reimaging the individual VM instances. With Ephemeral OS disk, 
you get lower read/write latency to the OS disk and faster VM reimage.

One of the ways to Scaleup and Sacledown is to create a time based Azure Fxn to sacle Azure Cosmos DB

Accounts in this organizational directory only - Select this option if you're building a line-of-business (LOB) application. 
This option is not available if you're not registering the application in a directory.
This option maps to Azure AD only single-tenant.
This is the default option unless you're registering the app outside of a directory. 
In cases where the app is registered outside of a directory, the default is Azure AD multi-tenant and personal Microsoft accounts.

-EnablePurgeProtection
If specified, protection against immediate deletion is enabled for this vault; requires soft delete to be enabled as well.
-EnableSoftDelete
Specifies that the soft-delete functionality is enabled for this key vault. When soft-delete is enabled, for a grace period, 
you can recover this key vault and its contents after it is deleted.

Managed service identity provides a secure way to access database.

X-Cache: TCP_MISS - This header is returned when there is a cache miss, and the content is served from the Origin.

allkeys-lru: evict keys by trying to remove the less recently used (LRU) keys first, in order to make space for the new data added.

To find out when people use your web app, what pages they're most interested in, where your users are located, and 
what browsers and operating systems they use. Analyze business and usage telemetry by using Azure Application Insights.

The Users, Sessions, and Events segmentation tool is used to slice and dice telemetry from your web app from three perspectives. 
By filtering and splitting the data, you can uncover insights about the relative usage of different pages and features.
• Users tool: How many people used your app and its features. Users are counted by using anonymous IDs stored in browser cookies. 
A single person using different browsers or machines will be counted as more than one user.
• Sessions tool: How many sessions of user activity have included certain pages and features of your app. 
A session is counted after half an hour of user inactivity, or after 24 hours of continuous use.
• Events tool: How often certain pages and features of your app are used. 
A page view is counted when a browser loads a page from your app, provided you have instrumented it.

You can use the daily volume cap to limit the data collected. However, if the cap is met, 
a loss of all telemetry sent from your application for the remainder of the day occurs. It is not advisable to have your application hit the daily cap. 
You can't track the health and performance of your application after it reaches the daily cap.
Instead of using the daily volume cap, use sampling to tune the data volume to the level you want.

In Application Insights, I only see a fraction of the events that are being generated by my app.
If you are consistently seeing the same fraction, it's probably due to adaptive sampling. 
To confirm this, open Search (from the overview blade) and look at an instance of a Request or other event. 
At the bottom of the properties section click "..." to get full property details. If Request Count > 1, then sampling is in operation.
Otherwise, it's possible that you're hitting a data rate limit for your pricing plan. These limits are applied per minute.

After signing in, you are able to access the information about the current user through the HttpContext.Current.User property.
While the server code has access to request headers, client code can access GET /.auth/me to get the same access tokens

Event Grid is an eventing backplane that enables event-driven, reactive programming. It uses a publish-subscribe model. 
Publishers emit events, but have no expectation about which events are handled. Subscribers decide which events they want to handle.
For example, an e-commerce site can use Service Bus to process the order, Event Grid to respond to events like an item was shipped.

Azure Event Hubs is a big data pipeline. It facilitates the capture, retention, and replay of telemetry and event stream data. 
The data can come from many concurrent sources. 
Event Hubs allows telemetry and event data to be made available to a variety of stream-processing infrastructures and analytics services. 
It is available either as data streams or bundled event batches. 
This service provides a single solution that enables rapid data retrieval for real-time processing as well as repeated replay of stored raw data.
The following scenarios are some of the scenarios where you can use Event Hubs:
Anomaly detection (fraud/outliers)
Application logging
Analytics pipelines, such as clickstreams
Live dashboarding
Archiving data
Transaction processing
User telemetry processing
Device telemetry streaming

Azure Event Hubs supports three protocols for consumers and producers: AMQP, Kafka, and HTTPS.

A namespace is a container for all messaging components. Multiple queues and topics can be in a single namespace, 
and namespaces often serve as application containers. A namespace can be compared to a "server" in the terminology of other brokers, 
but the concepts aren't directly equivalent.
Topics and subscriptions. Enable 1:n relationships between publishers and subscribers, 
allowing subscribers to select particular messages from a published message stream.
In this scenario, drivers subscribe to the restaurants.

Elements necessary to create ARM templates:schema, Content-Version,Resources

Bounded staleness is frequently chosen by globally distributed applications that expect low write latencies but require total global order guarantee. 
The reads are guaranteed to honor the consistent-prefix guarantee. 
The reads might lag behind writes by at most "K" versions (that is, "updates") of an item or by "T" time interval, whichever is reached first.

X-Cache: TCP_REMOTE_HIT - This header is returned when the content is served from the CDN regional cache (Origin shield layer)

Create an Application Insights resource
Create a URL ping test
Test frequency - Sets how often the test is run from each test location. 
With a default frequency of five minutes and five test locations, your site is tested on average every minute.

Authentication policies
Authenticate with Basic - Authenticate with a backend service using Basic authentication.
Authenticate with client certificate - Authenticate with a backend service using client certificates.
Authenticate with managed identity - Authenticate with the managed identity for the API Management service.



------------------------------------------------------------------------------------------------------------------------------------------------------------
MICROSOFT:
WebJobs are the only technology that permits developers to control retry policies.
Azure Logic Apps is the only one of the four technologies that provides a design-first approach intended for developers.
Azure Logic Apps is the only one of the four technologies that provides a design-first approach intended for developers.

The login notification is an event: it contains only a simple piece of status data and 
there is no expectation by the authentication service for the client applications to react to the notice in any particular way.
The delete-account notification is a message. The key factor is that the web service has an expectation about 
how the data layer will process the message: the data layer must remove the user's data from the database for the system to function correctly. 
Note that the message itself contains only simple information so this aspect of the communication could be considered an event; however, the fact that the web service requires the data layer to handle the notification in a specific way is sufficient to make this a message.

Use Service Bus topics if you:
Need multiple receivers to handle each message
Use Service Bus queues if you:
Need an At-Most-Once delivery guarantee.
Need a FIFO guarantee.
Need to group messages into transactions.
Want to receive messages without polling the queue.
Need to provide a role-based access model to the queues.
Need to handle messages larger than 64 KB but less than 256 KB.
Queue size will not grow larger than 80 GB.
Want to publish and consume batches of messages.
Queue storage isn't quite as feature rich, but if you don't need any of those features, it can be a simpler choice. 
In addition, it's the best solution if your app has any of the following requirements.

Use Queue storage if you:
Need an audit trail of all messages that pass through the queue.
Expect the queue to exceed 80 GB in size.
Want to track progress for processing a message inside of the queue.

There are several concepts in Azure Event Grid that connect a source to a subscriber:

Events: What happened.
Event sources: Where the event took place.
Topics: The endpoint where publishers send events.
Event subscriptions: The endpoint or built-in mechanism to route events, sometimes to multiple handlers. 
Subscriptions are also used by handlers to filter incoming events intelligently.
Event handlers: The app or service reacting to the event.

Each storage account has two access keys. This lets you follow the best-practice guideline of periodically replacing the key used by your applications without incurring downtime.
Possession of an access key identifies the account and grants you access. This is very similar to login credentials like a username and password.





